{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Feb  2 13:04:21 2019\n",
    "\n",
    "@author:\n",
    "\"\"\"\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from matplotlib import pyplot\n",
    "import pandas\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "NUMERICALS = ['GIAGE1', 'HA_HEIGHT', 'HA_WEIGHT', 'TUDRPRWK', 'B1FND', 'GRS_FN', 'HA_SMOKE', 'GIERACE_1.0',\n",
    "              'GIERACE_2.0', 'GIERACE_3.0', 'GIERACE_4.0', 'GIERACE_5.0', 'CLINIC_1.0','CLINIC_2.0', 'CLINIC_3.0',  \n",
    "              'CLINIC_4.0', 'CLINIC_5.0', 'CLINIC_6.0','NFWLKSPD_0.0', 'NFWLKSPD_1.0', 'NFWLKSPD_2.0']\n",
    "\n",
    "\n",
    "# change allele to number\n",
    "def allele_to_number(sample, attribute, sig_dict):\n",
    "    return sample[attribute].count(sig_dict[attribute])\n",
    "\n",
    "\n",
    "# fill numerical empty cells with median of the column, race with 1 (for white) and other categorical empty cells with the 0\n",
    "def fill_empty_cell(sample, attribute, data):\n",
    "    if pandas.isnull(sample[attribute]):\n",
    "        if attribute in NUMERICALS:\n",
    "            return data[attribute].median()\n",
    "        return int(attribute == 'GIERACE' )\n",
    "    elif attribute == 'FRAC':\n",
    "        return int(sample['FAANYSLD'] or sample['FAANYWST'] or sample['FAANYHIP'])\n",
    "    return sample[attribute]\n",
    "\n",
    "\n",
    "# calculate GRS\n",
    "def load_weight(sheet):\n",
    "    weight = []\n",
    "    df = pandas.read_excel('Estrada_63.xlsx', sheet_name=sheet)\n",
    "    w = list(df)[-1]\n",
    "    for i in range(len(df)):\n",
    "        weight.append(df.iloc[i][w])\n",
    "    return weight\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = pandas.read_excel('mros_1103snps_updated.xlsx')\n",
    "    # drop HA_SLDFXFU where only 10% is filled, drop subjectid,\n",
    "    data.drop(['HA_SLDFXFU', 'TURSMOKE', 'HA_SLDFX', 'HA_WRSTFX'], axis=1, inplace=True)\n",
    "    # make the fractures into 1 variable\n",
    "    data['FRAC'] = 0\n",
    "    for attribute in data.keys():\n",
    "        data[attribute] = data.apply(lambda sample: fill_empty_cell(sample, attribute, data), axis=1)\n",
    "    # drop the other fractured values\n",
    "    data.drop(['FAANYSLD', 'FAANYWST', 'FAANYHIP'], axis=1, inplace=True)\n",
    "    # encode the categorical data\n",
    "    data = pandas.DataFrame(pandas.get_dummies(data, columns=['GIERACE', 'CLINIC', 'NFWLKSPD', 'HA_SMOKE']))\n",
    "    features = list(data)[22:-6]\n",
    "    # setting Y and X\n",
    "    # Y_df = np.asarray(data['B1THD'], dtype=\"|S8\")\n",
    "    Y_df = data['B1FND']\n",
    "    X_df = pandas.read_excel('norma_continu_var.xlsx')\n",
    "    # weight_LS = load_weight('LS_sex-combined_beta')\n",
    "   # feature_data = data[features]\n",
    "    # weight_LS = pandas.DataFrame(pandas.Series(weight_LS, index=features, name=0))\n",
    "    #weight_FN = load_weight('FN_sex-combined_beta')\n",
    "    # X_df['GRS_LS'] = feature_data.dot(weight_LS)\n",
    "   # X_df['GRS_FN'] = feature_data.dot(weight_FN)\n",
    "   # X_df.drop(features, axis=1, inplace=True)\n",
    "    # print(Y_df.shape)\n",
    "    \n",
    "    #X_df.to_csv('X_variables.csv')\n",
    "    \n",
    "    print(Y_df.head())\n",
    "    print(X_df[1:1])\n",
    "    print(X_df.shape)\n",
    "    print(Y_df.shape)\n",
    "    # save data\n",
    "    with open('datamrosbmd1103_B1FND', 'wb') as data_file_handler:\n",
    "        import pickle\n",
    "\n",
    "        pickle.dump(\n",
    "            dict(\n",
    "                X=X_df,\n",
    "                Y=Y_df\n",
    "            ),\n",
    "            data_file_handler\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only continuous variables Standardization\n",
    "\n",
    "import numpy\n",
    "\n",
    "cv = pandas.read_excel('contious_variable.xlsx')\n",
    "    \n",
    "mean_cv = numpy.mean(cv, axis=0)\n",
    "std_cv = numpy.std(cv, axis=0)\n",
    "stan_cv = (cv - mean_cv) / std_cv  # mean variance normalization\n",
    "    \n",
    "stan_cv.to_csv('norma_continu_var.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the distribution plot for each BMD\n",
    "import pandas\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "data = pandas.read_excel('mros_1103snps_updated.xlsx')\n",
    "B1FND = data.astype(data['B1FND'])\n",
    "sns.distplot(B1FND)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from tensorflow import keras\n",
    "import pandas\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "with open('datamrosbmd1103_B1FND', 'rb') as file_handler:\n",
    "    data = pickle.load(file_handler)\n",
    "    X, Y = data.get('X', []).values, data.get('Y', []).values\n",
    "\n",
    "def linear_regression():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1125, input_dim=1125, kernel_initializer='normal', activation='relu', kernel_regularizer=keras.regularizers.l1(0.1)))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "    model.compile(optimizer='RMSprop', loss='mean_squared_error', metrics=['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(plot=True):\n",
    "    # fix random seed for reproducibility\n",
    "    seed = 7\n",
    "\n",
    "    # The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "    # numpy.random.seed(seed)\n",
    "    # The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "    # rn.seed(seed)\n",
    "\n",
    "    # according to keras documentation, numpy seed should be set before importing keras\n",
    "    # information regarding setup for obtaining reproducible results using Keras during development in the following link https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "\n",
    "    # The below tf.set_random_seed() will make random number generation in the TensorFlow backend have a well-defined initial state.\n",
    "    # tf.set_random_seed(seed)\n",
    "\n",
    "    batch_size = 50\n",
    "    # num_classes = 1\n",
    "    # epochs = 50\n",
    "    number_of_data = X.shape[0]\n",
    "    number_of_train_data = int(.8 * number_of_data)\n",
    "    # number_of_test_data = number_of_data - number_of_train_data\n",
    "\n",
    "    # load dataset\n",
    "    x_train, x_test = X[:number_of_train_data, :], X[number_of_train_data:, :]\n",
    "#     mean_train_data = numpy.mean(train_data, axis=0)\n",
    "#     std_train_data = numpy.std(train_data, axis=0)\n",
    "#     x_train = (train_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "#     x_test = (test_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "    y_train, y_test = Y[:number_of_train_data], Y[number_of_train_data:]\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    y_train = y_train.astype('float32')\n",
    "    y_test = y_test.astype('float32')\n",
    "\n",
    "    model = linear_regression()\n",
    "    # history = model.fit(x_train, y_train, batch_size=batch_size, epochs=3, verbose=1, validation_data=(x_test, y_test))\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, verbose=1, epochs=100, validation_data=(x_test, y_test))\n",
    "    print(history.history.keys())\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score)\n",
    "\n",
    "    score = model.evaluate(x_train, y_train, verbose=0)\n",
    "    print('Train loss:', score)\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "#     kfold = KFold(n_splits=10)\n",
    "#     estimator = KerasRegressor(build_fn=linear_regression, epochs=10, batch_size=5, verbose=0)\n",
    "#     results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "\n",
    "#    print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "    \n",
    "    print('Mean Squared Error of test: ', mean_squared_error(y_test, y_pred))\n",
    "    print('Mean Squared Error of train:', mean_squared_error(y_train, model.predict(x_train)))\n",
    "\n",
    "    print('Mean Absolute Error of test: ', mean_absolute_error(y_test, y_pred))\n",
    "    print('Mean Absolute Error of train: ', mean_absolute_error(y_train, model.predict(x_train)))\n",
    "\n",
    "    print('Coefficient of Determination for test: ', r2_score(y_test, y_pred))\n",
    "    print('Coefficient of Determination for train: ', r2_score(y_train, model.predict(x_train)))\n",
    "    \n",
    "    print('Explained variance score: ', explained_variance_score(y_test, y_pred))\n",
    "    \n",
    "    if not plot:\n",
    "        return history.history['loss'], history.history['val_loss']\n",
    "    pyplot.plot(history.history['loss'], 'b-')\n",
    "    pyplot.plot(history.history['val_loss'], 'r-')\n",
    "    pyplot.title('Mean Squared Error Loss: Linear Regression for Femoral Neck BMD')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['Train Data', 'Test Data'], loc='upper right')\n",
    "    pyplot.savefig('reg_B1FND_MSE')\n",
    "    pyplot.show()\n",
    "\n",
    "    # Plot the predicted value against the actual value\n",
    "    fig, ax = pyplot.subplots()\n",
    "    ax.scatter(y_test, y_pred)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_pred.min(), y_pred.max()], 'k--', lw=4)\n",
    "    pyplot.title('Scatter plot of Measured vs. Predicted : Linear Regression for Femoral Neck BMD')\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    pyplot.savefig('reg_B1FND_scatter')\n",
    "    pyplot.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "with open('datamrosbmd1103_B1FND', 'rb') as file_handler:\n",
    "    data = pickle.load(file_handler)\n",
    "    X, Y = data.get('X', []).values, data.get('Y', []).values\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "# seed = 7\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "# numpy.random.seed(seed)\n",
    "# The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "# random.seed(seed)\n",
    "\n",
    "# The below tensorflow.set_random_seed() will make random number generation in the TensorFlow backend have a well-defined initial state.\n",
    "# tensorflow.set_random_seed(seed)\n",
    "# Y = label_binarize(Y, classes=[0,1])\n",
    "\n",
    "batch_size = 120\n",
    "num_classes = 2\n",
    "\n",
    "number_of_data = X.shape[0]\n",
    "number_of_train_data = int(.8 * number_of_data)\n",
    "number_of_test_data = number_of_data - number_of_train_data\n",
    "\n",
    "x_train, x_test = X[:number_of_train_data, :], X[number_of_train_data:, :]\n",
    "# mean_train_data = numpy.mean(train_data, axis=0)\n",
    "# std_train_data = numpy.std(train_data, axis=0)\n",
    "# x_train = (train_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "# x_test = (test_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "y_train, y_test = Y[:number_of_train_data], Y[number_of_train_data:]\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "n_estimators = 100\n",
    "\n",
    "\n",
    "# override the RandomForestRegressor library\n",
    "class RandomForestRegressorCustom(RandomForestRegressor):\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        \"\"\"Returns the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "        The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
    "        The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "            Test samples. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator.\n",
    "\n",
    "        y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
    "            True values for X.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples], optional\n",
    "            Sample weights.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            R^2 of self.predict(X) wrt. y.\n",
    "        \"\"\"\n",
    "\n",
    "        return mean_squared_error(y, self.predict(X))\n",
    "\n",
    "\n",
    "def create_model(epoch):\n",
    "    return RandomForestRegressorCustom(n_estimators=epoch, random_state = 42, warm_start=True, oob_score=True, max_features='sqrt', max_depth=2)\n",
    "\n",
    "\n",
    "def main(plot=True):\n",
    "    epoch = 100\n",
    "    model = create_model(epoch)\n",
    "    model.fit(x_train, y_train)\n",
    "    model.score(x_test, y_test)\n",
    "    print(model.score(x_test, y_test), ' SOE')\n",
    "\n",
    "    train_score, test_score = [], []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        model = create_model(i + 1)\n",
    "        model.fit(x_train, y_train)\n",
    "        train_score.append(model.score(x_train, y_train))\n",
    "        test_score.append(model.score(x_test, y_test))\n",
    "#     print(test_score, ' TEST SCORE')\n",
    "#     print(train_score, ' TRAIN SCORE')\n",
    "\n",
    "#     importances = model.feature_importances_\n",
    "#     std = numpy.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "#              axis=0)\n",
    "#     indices = numpy.argsort(importances)[::-1]\n",
    "\n",
    "#     # Print the feature ranking\n",
    "#     print(\"Feature ranking:\")\n",
    "\n",
    "#     for f in range(X.shape[1]):\n",
    "#         print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "#     # Plot the feature importances of the forest\n",
    "#     pyplot.figure()\n",
    "#     pyplot.title(\"Feature importances\")\n",
    "#     pyplot.bar(range(X.shape[1]), importances[indices],\n",
    "#        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "#     pyplot.xticks(range(X.shape[1]), indices)\n",
    "#     pyplot.xlim([-1, X.shape[1]])\n",
    "#     pyplot.savefig('feature_importance_FNBMD')\n",
    "#     pyplot.show()\n",
    "  \n",
    "    feature_importances = pd.DataFrame(model.feature_importances_)\n",
    "    feature_importances.to_csv('feature_importances_FNBMD.csv')\n",
    "    \n",
    "    print('Mean Square Error of test: ', mean_squared_error(y_test, model.predict(x_test)))\n",
    "    print('Mean Square Error of train: ', mean_squared_error(y_train, model.predict(x_train)))\n",
    "\n",
    "    print('Mean Absolute Error of test: ', mean_absolute_error(y_test, model.predict(x_test)))\n",
    "    print('Mean Absolute Error of train: ', mean_absolute_error(y_train, model.predict(x_train)))\n",
    "    \n",
    "    print('Coefficient of Determination for test: ', r2_score(y_test, model.predict(x_test)))\n",
    "    print('Coefficient of Determination for train: ', r2_score(y_train, model.predict(x_train)))\n",
    "\n",
    "    if not plot:\n",
    "        return train_score, test_score\n",
    "    pyplot.plot(range(epoch), train_score, 'b-')\n",
    "    pyplot.plot(range(epoch), test_score, 'r-')\n",
    "    pyplot.title('Mean Squared Error Loss: Random Forest Regression for Femoral Neck BMD')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['Train Data', 'Test Data'], loc='upper right')\n",
    "    pyplot.savefig('rf_B1FND_MSE')\n",
    "    pyplot.show()\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Plot the predicted value against the actual value\n",
    "    fig, ax = pyplot.subplots()\n",
    "    ax.scatter(y_test, model.predict(x_test))\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_pred.min(), y_pred.max()], 'k--', lw=4)\n",
    "    pyplot.title('Scatter plot of Measured vs. Predicted : Random Forest for Femoral Neck BMD')\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    pyplot.savefig('rf_B1FND_scatter')\n",
    "    pyplot.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "import pickle\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "with open('datamrosbmd1103_B1FND', 'rb') as file_handler:\n",
    "    data = pickle.load(file_handler)\n",
    "    X, Y = data.get('X', []).values, data.get('Y', []).values\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    return RandomForestRegressor(n_estimators=35, verbose=1)\n",
    "\n",
    "\n",
    "def main(plot=True):\n",
    "    # fix random seed for reproducibility\n",
    "    # seed = 7\n",
    "    # The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "    # numpy.random.seed(seed)\n",
    "    # The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "    # random.seed(seed)\n",
    "\n",
    "    # The below tf.set_random_seed() will make random number generation in the TensorFlow backend have a well-defined initial state.\n",
    "    # tf.set_random_seed(seed)\n",
    "    # Y = label_binarize(Y, classes=[0,1])\n",
    "\n",
    "    # batch_size = 120\n",
    "    # num_classes = 2\n",
    "    # epochs = 15\n",
    "\n",
    "    number_of_data = X.shape[0]\n",
    "    number_of_train_data = int(.8 * number_of_data)\n",
    "    # number_of_test_data = number_of_data - number_of_train_data\n",
    "\n",
    "    # load dataset for MLP\n",
    "    x_train, x_test = X[:number_of_train_data, :], X[number_of_train_data:, :]\n",
    "#     mean_train_data = numpy.mean(train_data, axis=0)\n",
    "#     std_train_data = numpy.std(train_data, axis=0)\n",
    "#     x_train = (train_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "#     x_test = (test_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "    y_train, y_test = Y[:number_of_train_data], Y[number_of_train_data:]\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    validatescores = cross_val_score(model, x_train, y_train)\n",
    "    print(validatescores, ' ALL TRY')\n",
    "\n",
    "    history = model.fit(x_train, y_train)\n",
    "    print(history.feature_importances_)\n",
    "    print(history, ' HISTORY')\n",
    "    # y_pred = model.predict(x_test)\n",
    "\n",
    "    params = {'n_estimators': 100, 'max_depth': 2, 'min_samples_split': 2, 'learning_rate': 0.01, 'loss': 'ls'}\n",
    "    gbr = GradientBoostingRegressor(**params)\n",
    "    gbr.fit(x_train, y_train)\n",
    "\n",
    "    test_score = numpy.zeros((params['n_estimators'],), dtype=numpy.float64)\n",
    "    mse = mean_squared_error(y_test, gbr.predict(x_test))\n",
    "\n",
    "    print('Mean Square Error of test: ', mean_squared_error(y_test, gbr.predict(x_test)))\n",
    "    print('Mean Square Error of train: ', mean_squared_error(y_train, gbr.predict(x_train)))\n",
    "    \n",
    "    print('Mean Absolute Error of test: ', mean_absolute_error(y_test, gbr.predict(x_test)))\n",
    "    print('Mean Absolute Error of train: ', mean_absolute_error(y_train, gbr.predict(x_train)))\n",
    "    \n",
    "    print('Coefficient of Determination for test: ', r2_score(y_test, gbr.predict(x_test)))\n",
    "    print('Coefficient of Determination for train: ', r2_score(y_train, gbr.predict(x_train)))\n",
    "\n",
    "    for i, y_pred in enumerate(gbr.staged_predict(x_test)):\n",
    "        test_score[i] = gbr.loss_(y_test, y_pred)\n",
    "    if not plot:\n",
    "        return gbr.train_score_, test_score\n",
    "    pyplot.figure()\n",
    "    pyplot.title('Mean Square Error Loss: Gradient Boosting for Femoral Neck BMD')\n",
    "    pyplot.plot(numpy.arange(params['n_estimators']) + 1, gbr.train_score_, 'b-', label='Training')\n",
    "    pyplot.plot(numpy.arange(params['n_estimators']) + 1, test_score, 'r-', label='Test')\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.ylabel('Loss')\n",
    "    pyplot.savefig('gb_B1FND_MSE')\n",
    "    pyplot.show()\n",
    "\n",
    "    # Plot the predicted value against the actual value\n",
    "    fig, ax = pyplot.subplots()\n",
    "    ax.scatter(y_test, y_pred)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_pred.min(), y_pred.max()], 'k--', lw=4)\n",
    "    pyplot.title('Scatter plot of Measured vs. Predicted : Gradient Boosting for Femoral Neck BMD')\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    pyplot.savefig('gb_B1FND_scatter')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "\n",
    "with open('datamrosbmd1103_B1FND', 'rb') as file_handler:\n",
    "    data = pickle.load(file_handler)\n",
    "    X, Y = data.get('X', []).values, data.get('Y', []).values\n",
    "\n",
    "def mlp():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1125, input_dim=1125, kernel_initializer='normal', activation='relu', kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "    model.add(Dense(500, input_dim=500, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(plot=True):\n",
    "    # fix random seed for reproducibility\n",
    "    # seed = 7\n",
    "\n",
    "    # The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "    # numpy.random.seed(seed)\n",
    "    # The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "    # rn.seed(seed)\n",
    "\n",
    "    # according to keras documentation, numpy seed should be set before importing keras information regarding setup for obtaining reproducible results using Keras during development in the following link https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "\n",
    "    # The below tf.set_random_seed() will make random number generation in the TensorFlow backend have a well-defined initial state.\n",
    "    # tf.set_random_seed(seed)\n",
    "\n",
    "    batch_size = 50\n",
    "    # num_classes = 1\n",
    "    # epochs = 50\n",
    "    number_of_data = X.shape[0]\n",
    "    number_of_train_data = int(.8 * number_of_data)\n",
    "    # number_of_test_data = number_of_data - number_of_train_data\n",
    "\n",
    "    # load dataset\n",
    "    x_train, x_test = X[:number_of_train_data, :], X[number_of_train_data:, :]\n",
    "#     mean_train_data = numpy.mean(train_data, axis=0)\n",
    "#     std_train_data = numpy.std(train_data, axis=0)\n",
    "#     x_train = (train_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "#     x_test = (test_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "    y_train, y_test = Y[:number_of_train_data], Y[number_of_train_data:]\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    y_train = y_train.astype('float32')\n",
    "    y_test = y_test.astype('float32')\n",
    "\n",
    "    model = mlp()\n",
    "    # history = model.fit(x_train, y_train, batch_size=batch_size, epochs=3, verbose=1, validation_data=(x_test, y_test))\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, verbose=1, epochs=100, validation_data=(x_test, y_test))\n",
    "    print(history.history.keys())\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score)\n",
    "\n",
    "    score = model.evaluate(x_train, y_train, verbose=0)\n",
    "    print('Train loss:', score)\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    print('Mean Squared Error of test: ', mean_squared_error(y_test, y_pred))\n",
    "    print('Mean Squared Error of train: ', mean_squared_error(y_train, model.predict(x_train)))\n",
    "\n",
    "    print('Mean Absolute Error of test: ', mean_absolute_error(y_test, y_pred))\n",
    "    print('Mean Absolute Error of train: ', mean_absolute_error(y_train, model.predict(x_train)))\n",
    "\n",
    "    print('Coefficient of Determination for test: ', r2_score(y_test, y_pred))\n",
    "    print('Coefficient of Determination for train: ', r2_score(y_train, model.predict(x_train)))\n",
    "\n",
    "    if not plot:\n",
    "        return history.history['loss'], history.history['val_loss']\n",
    "    pyplot.plot(history.history['loss'], 'b-')\n",
    "    pyplot.plot(history.history['val_loss'], 'r-')\n",
    "    pyplot.title('Mean Squared Error Loss: MLP for Femoral Neck BMD')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['Train Data', 'Test Data'], loc='upper right')\n",
    "    pyplot.savefig('mlp_B1FND_MSE')\n",
    "    pyplot.show()\n",
    "    \n",
    "    # Plot the predicted value against the actual value\n",
    "    fig, ax = pyplot.subplots()\n",
    "    ax.scatter(y_test, y_pred)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_pred.min(), y_pred.max()], 'k--', lw=4)\n",
    "    pyplot.title('Scatter plot of Measured vs. Predicted : MLP for Femoral Neck BMD')\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    pyplot.savefig('mlp_B1FND_scatter')\n",
    "    pyplot.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of regression ML model by using 5 * 2 cv paired t test for linear vs. random forest\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "linear = LinearRegression()\n",
    "rf = RandomForestRegressor()\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "t, p = paired_ttest_5x2cv(estimator1=linear,\n",
    "                          estimator2=rf,\n",
    "                          X=X, y=Y,\n",
    "                          random_seed=1)\n",
    "\n",
    "print(\"t statistic: %.5f\" % t)\n",
    "print(\"p avlue: %.5f\" % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of regression ML model by using 5 * 2 cv paired t test for linear vs. gb\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "linear = LinearRegression()\n",
    "rf = RandomForestRegressor()\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "t, p = paired_ttest_5x2cv(estimator1=linear,\n",
    "                          estimator2=gb,\n",
    "                          X=X, y=Y,\n",
    "                          random_seed=2)\n",
    "\n",
    "print(\"t statistic: %.5f\" % t)\n",
    "print(\"p avlue: %.5f\" % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of regression ML model by using 5 * 2 cv paired t test for rf vs. gb\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "linear = LinearRegression()\n",
    "rf = RandomForestRegressor()\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "t, p = paired_ttest_5x2cv(estimator1=rf,\n",
    "                          estimator2=gb,\n",
    "                          X=X, y=Y,\n",
    "                          random_seed=1)\n",
    "\n",
    "print(\"t statistic: %.5f\" % t)\n",
    "print(\"p avlue: %.5f\" % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of regression ML model by using 5 * 2 cv paired t test for linear vs. mlp\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras import optimizers\n",
    "def mlp():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=21, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(20, input_dim=21, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(1, input_dim=21, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "mlp = mlp()\n",
    "linear = LinearRegression()\n",
    "rf = RandomForestRegressor()\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "t, p = paired_ttest_5x2cv(estimator1=linear,\n",
    "                          estimator2=mlp,\n",
    "                          X=X, y=Y,\n",
    "                          random_seed=2)\n",
    "\n",
    "print(\"t statistic: %.5f\" % t)\n",
    "print(\"p avlue: %.5f\" % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of regression ML model by using 5 * 2 cv paired t test for rf vs. mlp\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras import optimizers\n",
    "def mlp():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=21, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(20, input_dim=21, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(1, input_dim=21, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "mlp = mlp()\n",
    "linear = LinearRegression()\n",
    "rf = RandomForestRegressor()\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "t, p = paired_ttest_5x2cv(estimator1=rf,\n",
    "                          estimator2=mlp,\n",
    "                          X=X, y=Y,\n",
    "                          random_seed=2)\n",
    "\n",
    "print(\"t statistic: %.5f\" % t)\n",
    "print(\"p avlue: %.5f\" % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of regression ML model by using 5 * 2 cv paired t test for gb vs. mlp\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras import optimizers\n",
    "def mlp():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=21, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(20, input_dim=21, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(1, input_dim=21, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "mlp = mlp()\n",
    "linear = LinearRegression()\n",
    "rf = RandomForestRegressor()\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "t, p = paired_ttest_5x2cv(estimator1=gb,\n",
    "                          estimator2=mlp,\n",
    "                          X=X, y=Y,\n",
    "                          random_seed=2)\n",
    "\n",
    "print(\"t statistic: %.5f\" % t)\n",
    "print(\"p avlue: %.5f\" % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
